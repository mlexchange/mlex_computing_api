import argparse
import json
import logging
import subprocess
import time

import docker
import requests
import urllib

from model import MlexWorker, MlexJob, Status


def init_logging():
    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s',
                        level=logging.INFO)


def get_worker(worker_uid):
    '''
    Gets worker status
    Args:
        worker_uid:    Worker UID
    Returns:
        worker:        [MlexWorker]
    '''
    response = urllib.request.urlopen(f'{COMP_API_URL}workers/{worker_uid}')
    worker = json.loads(response.read())
    return MlexWorker.parse_obj(worker)


def get_job(job_uid):
    '''
    Gets the next job in worker
    Args:
        job_uid:    Job UID
    Returns:
        job:        [MlexJob]
    '''
    response = urllib.request.urlopen(f'{COMP_API_URL}jobs/{job_uid}')
    job = json.loads(response.read())
    job = MlexJob.parse_obj(job)
    return job


def get_next_job(job_uid):
    '''
    Gets the next job in worker
    Args:
        job_uid:    Job UID
    Returns:
        job:        [MlexJob]
    '''
    response = urllib.request.urlopen(f'{COMP_API_URL}private/jobs/{job_uid}')
    job = json.loads(response.read())
    if job:
        job = MlexJob.parse_obj(job)
        logging.info(f'Executing job: {job.uid}')
    return job


def update_job_status(job_id, status=None, logs=None):
    '''
    Updates the status of a given job
    Args:
        job_id:     Job UID
        status:     [Status]
        logs:       Job logs
    Returns:
        None
    '''
    json = None
    params = None
    if status:
        json = status.dict()
    if logs:
        params = {'logs': logs}
    response = requests.patch(f'{COMP_API_URL}private/jobs/{job_id}/update', params=params, json=json)
    if status:
        logging.info(f'\"Update job {job_id} with status {status.state}\" {response.status_code}')
    else:
        logging.info(f'\"Update job {job_id} logs\" {response.status_code}')
    pass


def check_assets(container_name, new_job_uid):
    '''
    Checks the list of assets that were generated by the job and forwards it to the content registry
    Args:
        container_name:     Docker container name
        new_job_uid:        Job UID in database
    Returns:
        None
    '''
    try:
        subprocess.run(["docker", "cp", f"{container_name}:/tmp/file_record_init.txt",
                        "/tmp/file_record_init.txt"])
        subprocess.run(["docker", "cp", f"{container_name}:/tmp/file_record_final.txt",
                        "/tmp/file_record_final.txt"])
        init_list = open('/tmp/file_record_init.txt', 'r').read().splitlines()
        final_list = open('/tmp/file_record_final.txt', 'r').read().splitlines()
        if len(init_list)<len(final_list):
            assets = set(init_list[:-2]) ^ set(final_list[:-2])
            assets = [x for x in assets if not '__pycache__' in x]
            # the next line should be replaced with the POST command to register the assets in the model registry
            logging.info(f'Update job {new_job_uid} generated the following assets {assets}')
    except Exception as err:
        logging.error(f'No assets for {new_job_uid}: {err}')
    pass


COMP_API_URL = 'http://job-service:8080/api/v0/'
DOCKER_CLIENT = docker.from_env()


if __name__ == '__main__':
    init_logging()

    parser = argparse.ArgumentParser()
    parser.add_argument('worker', help='worker description')
    args = parser.parse_args()

    # get worker information
    worker = MlexWorker(**json.loads(args.worker))
    num_processors = worker.requirements.num_processors
    list_gpus = worker.requirements.list_gpus

    for job_uid in worker.jobs_list:
        new_job = get_next_job(job_uid)
        if new_job:
            try:        # launch job
                logs = ''
                docker_job = new_job.job_kwargs
                cmd = docker_job.cmd
                volumes = []
                if len(new_job.working_directory)>0:
                    volumes = ['{}:/app/work/data'.format(new_job.working_directory)]
                    cmd = f"tree -ifo /tmp/file_record_init.txt ; {cmd} ; tree -ifo /tmp/file_record_final.txt"
                    cmd = f'bash -c {json.dumps(cmd)}'
                else:
                    file_record = None              # Not reporting assets
                ports = {}
                if docker_job.port:
                    for port in docker_job.port:
                        ports[f'{port}/tcp'] = None     # assigns random port
                device_requests = []
                if len(list_gpus)>0:
                    device_requests=[docker.types.DeviceRequest(device_ids=list_gpus,
                                                                capabilities=[['gpu']]
                                                                )],
                container = DOCKER_CLIENT.containers.run(docker_job.uri,
                                                         cpu_count=num_processors,
                                                         device_requests=device_requests,
                                                         command=cmd,
                                                         ports=ports,
                                                         volumes=volumes,
                                                         detach=True)
            except Exception as err:
                logging.error(f'Job {new_job.uid} failed: {str(err)}')
                update_job_status(new_job.uid, status=Status(state="failed", return_code=str(err)))
            else:
                container.reload()      # to get the ports
                logging.info(f'Job {new_job.uid} running: {container.ports}')
                while container.status == 'created' or container.status == 'running':
                    new_job = get_job(job_uid)      # gets current state of the job in database to check if terminated
                    if new_job.terminate:
                        output = container.logs(stdout=True)            # retrieve last logs and outputs
                        container.kill()                                # kill container
                        update_job_status(new_job.uid, status=Status(state="terminated"))
                    else:
                        try:
                            # retrieve logs and check if new assets have been created
                            tmp_logs = container.logs(stdout=True)
                            if logs != tmp_logs:
                                logs = tmp_logs
                                update_job_status(new_job.uid, logs=logs)
                        except Exception as err:
                            logging.error(f'Job {new_job.uid} failed: {str(err)}')
                            update_job_status(new_job.uid, status=Status(state="failed", return_code=str(err)))
                    time.sleep(1)
                    container = DOCKER_CLIENT.containers.get(container.id)
                result = container.wait()
                if result["StatusCode"] == 0:
                    logs = container.logs(stdout=True)
                    update_job_status(new_job.uid, status=Status(state="complete"), logs=logs)
                    if len(new_job.working_directory) > 0:
                        check_assets(container.name, new_job.uid)
                else:
                    if new_job.terminate is None:
                        try:
                            output = container.logs(stdout=True)
                            update_job_status(new_job.uid, logs=logs)
                            check_assets(container, new_job.uid)
                        except Exception:
                            pass
                        err = "Code: "+str(result["StatusCode"])+ " Error: " + repr(result["Error"])
                        logging.error(f'Job {new_job.uid} failed: {err}')
                        update_job_status(new_job.uid, status=Status(state="failed", return_code=err))
                # container.remove()
